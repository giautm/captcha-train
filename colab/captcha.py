# -*- coding: utf-8 -*-
"""Captcha2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C__3MR7QEo9CqFKHbCMRMdbttS4R4hyS
"""

# Commented out IPython magic to ensure Python compatibility.
# %rm -rf /content/__MACOSX
# %rm -rf /content/data
# %mkdir /content/data

!unzip -qq /content/drive/MyDrive/captcha/debug-10k.zip
# %mv /content/debug-10k /content/data/train
!echo "Done train Data"

!unzip -qq /content/drive/MyDrive/captcha/debug-2k.zip
# %mv /content/debug-2k /content/data/validation
!echo "Done validation Data"

# %rm -rf /content/__MACOSX

"""Mount data từ Google Drive và giải nén vào folder đích

Load dataset vào bộ nhớ để chia thành 2 tập train và validation
"""

import tensorflow as tf
from tensorflow.keras import preprocessing

BATCH_SIZE = 128
COLOR_MODE = "rgba"
IMG_HEIGHT = 50
IMG_WIDTH = 180

train_ds = preprocessing.image_dataset_from_directory(
  directory="data/train",
  label_mode="categorical",
  seed=123,
  color_mode=COLOR_MODE,
  image_size=(IMG_HEIGHT, IMG_WIDTH),
  batch_size=BATCH_SIZE)

validation_ds = preprocessing.image_dataset_from_directory(
  directory="data/validation",
  label_mode="categorical",
  seed=123,
  color_mode=COLOR_MODE,
  image_size=(IMG_HEIGHT, IMG_WIDTH),
  batch_size=BATCH_SIZE)

# Configure the dataset for performance
def configure_for_performance(ds):
    ds = ds.cache()
    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    return ds

train_ds = configure_for_performance(train_ds)
validation_ds = configure_for_performance(validation_ds)

from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow.keras import optimizers
from tensorflow.keras import Sequential

MODEL_OUTPUT_DIR="/content/drive/MyDrive/captcha/captcha-model"
CLASS_NO = 23
EPOCHS = 120

def get_model():
    m = Sequential([
      layers.experimental.preprocessing.Rescaling(1./255, name=""),

      layers.Conv2D(32, (3, 3), padding="same", input_shape=(IMG_HEIGHT, IMG_WIDTH, 1)),
      layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
      layers.Conv2D(64, (3, 3), padding="same"),
      layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
      layers.Conv2D(128, (3, 3), padding="same"),
      layers.Conv2D(64, (1, 1), padding="same"),
      layers.Conv2D(128, (3, 3), padding="same"),
      layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
      layers.Conv2D(256, (3, 3), padding="same"),
      layers.Conv2D(128, (1, 1), padding="same"),
      layers.Conv2D(256, (3, 3), padding="same"),
      layers.Conv2D(128, (1, 1), padding="same"),
      layers.Conv2D(256, (3, 3), padding="same"),
      layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
      layers.Conv2D(512, (3, 3), padding="same"),
      layers.Conv2D(256, (1, 1), padding="same"),
      layers.Conv2D(512, (3, 3), padding="same"),
      layers.Conv2D(256, (1, 1), padding="same"),
      layers.Conv2D(512, (3, 3), padding="same"),
      layers.Conv2D(256, (1, 1), padding="same"),
      layers.Conv2D(512, (3, 3), padding="same"),
      layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
      layers.Flatten(),
      layers.Dropout(rate=0.5),
      layers.Dense(CLASS_NO, activation="softmax", name="output"),
    ])
    m.compile(
      loss=losses.CategoricalCrossentropy(),
      metrics=['accuracy'],
      optimizer=optimizers.Adam(learning_rate=0.00001),
    )
    return m

model=get_model()
model.fit(train_ds, epochs=EPOCHS, validation_data=validation_ds, verbose=1)
model.summary()
model.save(MODEL_OUTPUT_DIR)